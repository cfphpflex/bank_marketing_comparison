# -*- coding: utf-8 -*-
"""UCB_17_Compare_Classifiers_Bank.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dvimksfHCpbZ9t2wCWVLRrZ2yp0xc0zV
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE


import time

# Load dataset
data_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip"
!wget -q $data_url -O bank.zip
!unzip -o bank.zip

df = pd.read_csv("bank.csv", sep=';')
df.head()

## Data Exploration and Cleaning
print(df.info())
print(df.isnull().sum())

df.replace('unknown', pd.NA, inplace=True)
print(df.isna().sum())
# Optional: Drop rows or impute after checking counts

sns.boxplot(data=df[['age', 'duration', 'balance']])
plt.show()
# Consider capping or filtering extreme outliers if needed

df = df.drop_duplicates()

df.drop(columns='duration', inplace=True)

#Class Imbalance MitigationClass weighting:

LogisticRegression(class_weight='balanced')

# Label Encoding for binary target variable
le = LabelEncoder()
df['y'] = le.fit_transform(df['y'])

# Convert categorical variables to dummies
df_encoded = pd.get_dummies(df.drop('y', axis=1), drop_first=True)
df_encoded['y'] = df['y']

# X, y data
X = df_encoded.drop('y', axis=1)
y = df_encoded['y']

# Encode binary target
le = LabelEncoder()
df['y'] = le.fit_transform(df['y'])  # 'yes' → 1, 'no' → 0

# One-hot encode categorical features
df_encoded = pd.get_dummies(df.drop('y', axis=1), drop_first=True)
df_encoded['y'] = df['y']

# Define features and target
X = df_encoded.drop('y', axis=1)
y = df_encoded['y']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 	SMOTE oversampling balance
# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply SMOTE oversampling
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

## Modeling and Evaluation
models = {
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "SVM": SVC(probability=True)
}

results = {}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    score = cross_val_score(model, X_train_scaled, y_train, cv=5).mean()
    results[name] = {
        "CV Score": score,
        "Report": classification_report(y_test, y_pred, output_dict=True),
        "Confusion Matrix": confusion_matrix(y_test, y_pred)
    }

# Print summary
for name, metrics in results.items():
    print(f"\nModel: {name}")
    print(f"Cross-Validation Score: {metrics['CV Score']:.4f}")
    print("Classification Report:")
    print(pd.DataFrame(metrics['Report']).transpose())
    print("Confusion Matrix:")
    print(metrics['Confusion Matrix'])

# Tree, HyperParams & fit
# Example: A small tree to demonstrate underfitting/overfitting control
tree_overfit = DecisionTreeClassifier(max_depth=2, min_samples_split=2, min_samples_leaf=1, random_state=42)
tree_overfit.fit(X_train, y_train)

# SCORE
train_acc = tree_overfit.score(X_train, y_train)
test_acc = tree_overfit.score(X_test, y_test)
print(f"\nDecision Tree (depth=2) Train Accuracy: {train_acc:.4f}")
print(f"Decision Tree (depth=2) Test Accuracy: {test_acc:.4f}")

# ROC Curve for best model (example: Logistic Regression)
best_model = LogisticRegression(max_iter=1000)
best_model.fit(X_train_scaled, y_train)
y_probs = best_model.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_probs)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label='Logistic Regression')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

df = pd.get_dummies(df, drop_first=True)

print(df['y'].value_counts(normalize=True))  # Class balance
print(df.corr())

print(df.describe())
print(df['y'].value_counts())
print(df.corr(numeric_only=True))
for col in ['job', 'education', 'marital', 'contact', 'poutcome']:
    print(df[col].value_counts())

for col in ['balance', 'age']:
    upper = df[col].quantile(0.99)
    df[col] = np.where(df[col] > upper, upper, df[col])

df['age_group'] = pd.cut(df['age'], bins=[18, 30, 40, 50, 60, 100], labels=['18-30', '31-40', '41-50', '51-60', '60+'])
df['has_prev_contact'] = (df['previous'] > 0).astype(int)

for col in ['job', 'education', 'marital', 'contact', 'poutcome']:
    df[col] = df[col].astype('category')

# Bar plot example
sns.countplot(x='job', hue='y', data=df)
plt.show()
# Correlation heatmap
sns.heatmap(df.corr(numeric_only=True), annot=True)
plt.show()

